{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from langdetect import detect\n",
        "from nltk import pos_tag\n",
        "\n",
        "def clean_html_tags(text):\n",
        "    clean_text = re.sub('<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n",
        "def normalize_unicode(text):\n",
        "    clean_text = ''.join(c for c in unicodedata.normalize('NFC', text) if c <= '\\uFFFF')\n",
        "    return clean_text\n",
        "\n",
        "def correct_fat_finger(text):\n",
        "    blob = TextBlob(text)\n",
        "    corrected_text = str(blob.correct())\n",
        "    return corrected_text\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "def tokenize_words(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    return words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "def stem_words(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return lemmatized_words\n",
        "\n",
        "def remove_punctuation_digits(text):\n",
        "    cleaned_text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
        "    return cleaned_text\n",
        "\n",
        "def convert_to_lowercase(text):\n",
        "    lowercased_text = text.lower()\n",
        "    return lowercased_text\n",
        "\n",
        "def detect_language(text):\n",
        "    language = detect(text)\n",
        "    return language\n",
        "\n",
        "def pos_tagging(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(words)\n",
        "    return pos_tags\n",
        "\n",
        "# Example :\n",
        "input_text = \"<html><body>Hello, world! ðŸ˜Š</body></html>\"\n",
        "cleaned_text = clean_html_tags(input_text)\n",
        "normalized_text = normalize_unicode(cleaned_text)\n",
        "corrected_text = correct_fat_finger(normalized_text)\n",
        "sentences = tokenize_sentences(corrected_text)\n",
        "words = tokenize_words(sentences[0])\n",
        "filtered_words = remove_stopwords(words)\n",
        "stemmed_words = stem_words(filtered_words)\n",
        "lemmatized_words = lemmatize_words(filtered_words)\n",
        "cleaned_text_no_punct_digits = remove_punctuation_digits(corrected_text)\n",
        "lowercased_text = convert_to_lowercase(cleaned_text)\n",
        "language = detect_language(cleaned_text)\n",
        "\n",
        "print(\"Original Text:\", input_text)\n",
        "print(\"Cleaned Text:\", cleaned_text)\n",
        "print(\"Normalized Text:\", normalized_text)\n",
        "print(\"Corrected Text:\", corrected_text)\n",
        "print(\"Tokenized Sentences:\", sentences)\n",
        "print(\"Tokenized Words:\", words)\n",
        "print(\"Words without Stopwords:\", filtered_words)\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n",
        "print(\"Text without Punctuation and Digits:\", cleaned_text_no_punct_digits)\n",
        "print(\"Lowercased Text:\", lowercased_text)\n",
        "print(\"Detected Language:\", language)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAiWiXBkG23R",
        "outputId": "4addaa0c-3324-49b4-8883-8be9aaa91770"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: <html><body>Hello, world! ðŸ˜Š</body></html>\n",
            "Cleaned Text: Hello, world! ðŸ˜Š\n",
            "Normalized Text: Hello, world! \n",
            "Corrected Text: Hello, world! \n",
            "Tokenized Sentences: ['Hello, world!']\n",
            "Tokenized Words: ['Hello', ',', 'world', '!']\n",
            "Words without Stopwords: ['Hello', ',', 'world', '!']\n",
            "Stemmed Words: ['hello', ',', 'world', '!']\n",
            "Lemmatized Words: ['Hello', ',', 'world', '!']\n",
            "Text without Punctuation and Digits: Hello world \n",
            "Lowercased Text: hello, world! ðŸ˜Š\n",
            "Detected Language: en\n"
          ]
        }
      ]
    }
  ]
}